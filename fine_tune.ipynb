{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import Audio\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc29a8cc58446ebcbc9bc56be9b024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in csv and take 50% as training data flemish and dutch\n",
    "df = pd.read_csv('./metadata/cgn_cd_result_merge_meta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Flemish and Dutch\n",
    "flemish = df[df['region'].notnull()]  # Filter where 'region' is not NaN\n",
    "dutch = df[df['region'].isnull()]     # Filter where 'region' is NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of training data (50% of each)\n",
    "flemish_train_size = int(0.5 * len(flemish))\n",
    "dutch_train_size = int(0.5 * len(dutch))\n",
    "\n",
    "# Take the first 50% as training data\n",
    "flemish_train = flemish[:flemish_train_size]\n",
    "dutch_train = dutch[:dutch_train_size]\n",
    "\n",
    "# Filter by Flemish and Dutch\n",
    "train_data_vlnl = pd.concat([flemish_train, dutch_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flemish_eval = flemish[flemish_train_size:]\n",
    "dutch_eval = dutch[dutch_train_size:]\n",
    "\n",
    "eval_data = pd.concat([flemish_eval, dutch_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vlnl =  train_data_vlnl[[\"wav_filename\", \"transcript\"]]\n",
    "eval_data = eval_data[[\"wav_filename\", \"transcript\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vlnl.columns = [\"audio\", \"sentence\"]\n",
    "eval_data.columns = [\"audio\", \"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## convert the pandas dataframes to dataset \n",
    "train_dataset = Dataset.from_pandas(train_data_vlnl)\n",
    "test_dataset = Dataset.from_pandas(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the sample rate of every audio files using cast_column function\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to create a WhisperProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    # compute log-Mel input features from input audio array \n",
    "    audio = examples[\"audio\"]\n",
    "    examples[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    del examples[\"audio\"]\n",
    "    sentences = examples[\"sentence\"]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
    "    del examples[\"sentence\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316e8bcf2778462e965a980090f076e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc978d5c7fe4dfba15a1315f6f6355b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8617 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(prepare_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"dutch\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Data Collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./JensCoet/whisper-small-nl\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=1,\n",
    "    max_steps=2000,  # set max steps to > 2k\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,   # set to < max_steps\n",
    "    eval_steps=500,  # set to < max_steps\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# Now, initialize the trainer with the processed audio data\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following took 1080min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a876ceb1b24944969e17cb14f17dda2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:697: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3742, 'learning_rate': 9.884942471235619e-06, 'epoch': 0.05}\n",
      "{'loss': 0.9085, 'learning_rate': 9.759879939969985e-06, 'epoch': 0.09}\n",
      "{'loss': 0.8672, 'learning_rate': 9.634817408704354e-06, 'epoch': 0.14}\n",
      "{'loss': 0.8171, 'learning_rate': 9.50975487743872e-06, 'epoch': 0.19}\n",
      "{'loss': 0.7356, 'learning_rate': 9.384692346173087e-06, 'epoch': 0.23}\n",
      "{'loss': 0.7586, 'learning_rate': 9.259629814907455e-06, 'epoch': 0.28}\n",
      "{'loss': 0.6844, 'learning_rate': 9.134567283641822e-06, 'epoch': 0.32}\n",
      "{'loss': 0.6553, 'learning_rate': 9.009504752376189e-06, 'epoch': 0.37}\n",
      "{'loss': 0.661, 'learning_rate': 8.884442221110557e-06, 'epoch': 0.42}\n",
      "{'loss': 0.6048, 'learning_rate': 8.759379689844924e-06, 'epoch': 0.46}\n",
      "{'loss': 0.5899, 'learning_rate': 8.63431715857929e-06, 'epoch': 0.51}\n",
      "{'loss': 0.6135, 'learning_rate': 8.509254627313657e-06, 'epoch': 0.56}\n",
      "{'loss': 0.5615, 'learning_rate': 8.384192096048025e-06, 'epoch': 0.6}\n",
      "{'loss': 0.5084, 'learning_rate': 8.259129564782392e-06, 'epoch': 0.65}\n",
      "{'loss': 0.4755, 'learning_rate': 8.134067033516759e-06, 'epoch': 0.7}\n",
      "{'loss': 0.4559, 'learning_rate': 8.009004502251127e-06, 'epoch': 0.74}\n",
      "{'loss': 0.4735, 'learning_rate': 7.883941970985494e-06, 'epoch': 0.79}\n",
      "{'loss': 0.4354, 'learning_rate': 7.75887943971986e-06, 'epoch': 0.84}\n",
      "{'loss': 0.4141, 'learning_rate': 7.633816908454229e-06, 'epoch': 0.88}\n",
      "{'loss': 0.4438, 'learning_rate': 7.508754377188595e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be86bcd2edb74d7db972719275bcd1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9492278099060059, 'eval_wer': 117.07920639708462, 'eval_runtime': 8708.6343, 'eval_samples_per_second': 0.989, 'eval_steps_per_second': 0.989, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4124, 'learning_rate': 7.383691845922962e-06, 'epoch': 0.97}\n",
      "{'loss': 0.3163, 'learning_rate': 7.258629314657329e-06, 'epoch': 1.02}\n",
      "{'loss': 0.231, 'learning_rate': 7.133566783391697e-06, 'epoch': 1.07}\n",
      "{'loss': 0.2374, 'learning_rate': 7.0085042521260636e-06, 'epoch': 1.11}\n",
      "{'loss': 0.2286, 'learning_rate': 6.883441720860431e-06, 'epoch': 1.16}\n",
      "{'loss': 0.2262, 'learning_rate': 6.758379189594798e-06, 'epoch': 1.21}\n",
      "{'loss': 0.2023, 'learning_rate': 6.633316658329165e-06, 'epoch': 1.25}\n",
      "{'loss': 0.2094, 'learning_rate': 6.508254127063533e-06, 'epoch': 1.3}\n",
      "{'loss': 0.1931, 'learning_rate': 6.383191595797899e-06, 'epoch': 1.35}\n",
      "{'loss': 0.2039, 'learning_rate': 6.258129064532267e-06, 'epoch': 1.39}\n",
      "{'loss': 0.1775, 'learning_rate': 6.1330665332666335e-06, 'epoch': 1.44}\n",
      "{'loss': 0.1748, 'learning_rate': 6.008004002001001e-06, 'epoch': 1.49}\n",
      "{'loss': 0.1834, 'learning_rate': 5.8829414707353685e-06, 'epoch': 1.53}\n",
      "{'loss': 0.1741, 'learning_rate': 5.757878939469735e-06, 'epoch': 1.58}\n",
      "{'loss': 0.1655, 'learning_rate': 5.632816408204103e-06, 'epoch': 1.62}\n",
      "{'loss': 0.1628, 'learning_rate': 5.50775387693847e-06, 'epoch': 1.67}\n",
      "{'loss': 0.1453, 'learning_rate': 5.382691345672837e-06, 'epoch': 1.72}\n",
      "{'loss': 0.1467, 'learning_rate': 5.257628814407204e-06, 'epoch': 1.76}\n",
      "{'loss': 0.1407, 'learning_rate': 5.132566283141571e-06, 'epoch': 1.81}\n",
      "{'loss': 0.1451, 'learning_rate': 5.0075037518759384e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2a19ab932348788fdf19e7c0e6fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.028854489326477, 'eval_wer': 104.31643161853874, 'eval_runtime': 11731.5389, 'eval_samples_per_second': 0.735, 'eval_steps_per_second': 0.735, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1348, 'learning_rate': 4.882441220610306e-06, 'epoch': 1.9}\n",
      "{'loss': 0.1334, 'learning_rate': 4.757378689344673e-06, 'epoch': 1.95}\n",
      "{'loss': 0.1095, 'learning_rate': 4.63231615807904e-06, 'epoch': 2.0}\n",
      "{'loss': 0.0725, 'learning_rate': 4.507253626813407e-06, 'epoch': 2.04}\n",
      "{'loss': 0.0606, 'learning_rate': 4.382191095547774e-06, 'epoch': 2.09}\n",
      "{'loss': 0.0529, 'learning_rate': 4.257128564282142e-06, 'epoch': 2.14}\n",
      "{'loss': 0.0561, 'learning_rate': 4.132066033016508e-06, 'epoch': 2.18}\n",
      "{'loss': 0.0554, 'learning_rate': 4.007003501750876e-06, 'epoch': 2.23}\n",
      "{'loss': 0.055, 'learning_rate': 3.8819409704852425e-06, 'epoch': 2.27}\n",
      "{'loss': 0.0549, 'learning_rate': 3.75687843921961e-06, 'epoch': 2.32}\n",
      "{'loss': 0.0513, 'learning_rate': 3.631815907953977e-06, 'epoch': 2.37}\n",
      "{'loss': 0.0527, 'learning_rate': 3.506753376688344e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0488, 'learning_rate': 3.3816908454227117e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0493, 'learning_rate': 3.2566283141570788e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0492, 'learning_rate': 3.131565782891446e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0421, 'learning_rate': 3.006503251625813e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0416, 'learning_rate': 2.8814407203601804e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0419, 'learning_rate': 2.7563781890945475e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0433, 'learning_rate': 2.6313156578289145e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0433, 'learning_rate': 2.5062531265632816e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f86f0699d2947b499feb2dd44794ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.113805890083313, 'eval_wer': 103.94708493127074, 'eval_runtime': 16602.2241, 'eval_samples_per_second': 0.519, 'eval_steps_per_second': 0.519, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jensc\\Desktop\\BP_Models\\WhisperV3\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0404, 'learning_rate': 2.381190595297649e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0316, 'learning_rate': 2.256128064032016e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0346, 'learning_rate': 2.1310655327663833e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0315, 'learning_rate': 2.0060030015007508e-06, 'epoch': 2.97}\n",
      "{'loss': 0.0266, 'learning_rate': 1.8809404702351178e-06, 'epoch': 3.02}\n",
      "{'loss': 0.0201, 'learning_rate': 1.755877938969485e-06, 'epoch': 3.06}\n",
      "{'loss': 0.015, 'learning_rate': 1.6308154077038522e-06, 'epoch': 3.11}\n",
      "{'loss': 0.0161, 'learning_rate': 1.5057528764382193e-06, 'epoch': 3.16}\n",
      "{'loss': 0.0182, 'learning_rate': 1.3806903451725863e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0162, 'learning_rate': 1.2556278139069536e-06, 'epoch': 3.25}\n",
      "{'loss': 0.0165, 'learning_rate': 1.1305652826413207e-06, 'epoch': 3.3}\n",
      "{'loss': 0.016, 'learning_rate': 1.005502751375688e-06, 'epoch': 3.34}\n",
      "{'loss': 0.0145, 'learning_rate': 8.804402201100551e-07, 'epoch': 3.39}\n",
      "{'loss': 0.0148, 'learning_rate': 7.553776888444222e-07, 'epoch': 3.44}\n",
      "{'loss': 0.0153, 'learning_rate': 6.303151575787894e-07, 'epoch': 3.48}\n",
      "{'loss': 0.017, 'learning_rate': 5.052526263131566e-07, 'epoch': 3.53}\n",
      "{'loss': 0.0148, 'learning_rate': 3.8019009504752383e-07, 'epoch': 3.57}\n",
      "{'loss': 0.0154, 'learning_rate': 2.5512756378189096e-07, 'epoch': 3.62}\n",
      "{'loss': 0.0143, 'learning_rate': 1.3006503251625814e-07, 'epoch': 3.67}\n",
      "{'loss': 0.0129, 'learning_rate': 5.002501250625313e-09, 'epoch': 3.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e43888eb5a40b083318effbe3482b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1677577495574951, 'eval_wer': 103.5346477971548, 'eval_runtime': 8318.4558, 'eval_samples_per_second': 1.036, 'eval_steps_per_second': 1.036, 'epoch': 3.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 64288.8047, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.031, 'train_loss': 0.23456191690266132, 'epoch': 3.71}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.23456191690266132, metrics={'train_runtime': 64288.8047, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.031, 'train_loss': 0.23456191690266132, 'epoch': 3.71})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before finetuning:\n",
    "wer: 171 of -71wa\n",
    "after:\n",
    "wer: 103 of -3wa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
